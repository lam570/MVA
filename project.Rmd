title: "Project"
author: "Leon Masin"
date: "3/29/2024"
output: html_document
---


```{r}
library(psych)
library(readxl)
mva <- read_excel("C:/Users/Student/Downloads/social_media_cleaned.xlsx")
mva <- mva[-1]


#how far you are from the average student?
scale <- scale(mva)

classcov <- cor(mva)
classmean <- colMeans(mva)

mvascale <- mahalanobis(mva, classmean, classcov)
print(mvascale[1])

#Here I am the first row, and it shows that I am 452 units away from the average student, this is probably because I consume far more OTT content then the rest of my class, because of my love of film and desire to work in the entertainment industry.

#PCA Analysis
library(readr)
library(FactoMineR)
library("factoextra")

mva_pca <- prcomp(mva,scale=TRUE)
mva_pca
summary(mva_pca)
eigen_mva<- mva_pca$sdev^2

plot(eigen_mva, xlab = "Component number", ylab = "Component variance", type = "l", main = "Scree diagram")

res.pca <- PCA(mva, graph = FALSE)

fviz_pca_var(res.pca, col.var = "black")

plot(log(eigen_mva), xlab = "Component number",ylab = "log(Component variance)", type="l",main = "Log(eigenvalue) diagram")

#I would probably only include 6 dimensions since the Scree diagram has an elbow at PC6 and cumulatively Pc1-6 include 89 percent of the variance 

#Cluster Analysis

library(readr)
library(cluster)
library(factoextra)
library(magrittr)
library(NbClust)

attach(mva)

res.nbclust <- mva %>% scale() %>% NbClust(distance = "euclidean", min.nc = 2, max.nc = 10, method = "complete", index ="all") 

#Using Majority rule optimal amount of clusters is 2, will try using kmeans to see difference visually between 2/3 clusters

kmeans2.mva<- kmeans(scale,2,nstart = 10)
perc.var.2 <- round(100*(1 - kmeans2.mva$betweenss/kmeans2.mva$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2

(kmeans3.mva <- kmeans(scale,3,nstart = 10))
perc.var.3 <- round(100*(1 - kmeans3.mva$betweenss/kmeans3.mva$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3

(kmeans4.mva <- kmeans(scale,4,nstart = 10))
perc.var.4 <- round(100*(1 - kmeans4.mva$betweenss/kmeans4.mva$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4

(kmeans5.mva <- kmeans(scale,5,nstart = 10))
perc.var.5 <- round(100*(1 - kmeans5.mva$betweenss/kmeans5.mva$totss),1)
names(perc.var.5) <- "Perc. 5 clus"
perc.var.5

(kmeans6.mva <- kmeans(scale,6,nstart = 10))
perc.var.6 <- round(100*(1 - kmeans6.mva$betweenss/kmeans6.mva$totss),1)
names(perc.var.6) <- "Perc. 6 clus"
perc.var.6

Variance_List <- c(perc.var.2,perc.var.3, perc.var.4, perc.var.5, perc.var.6)
plot(Variance_List)

pam.res <- pam(mva, 2)
fviz_cluster(pam.res)

(agn.mva <- agnes(mva, metric="euclidean", stand=TRUE, method = "single"))
agn.mva$merge
plot(agn.mva, which.plots=2)

#Factor Analysis

library(psych)

fit.pc <- principal(mva, nfactors=4, rotate="varimax")
fit.pc


fit.pc <- principal(mva, nfactors=3, rotate="varimax")
fit.pc

fit.pc$loadings
fit.pc$scores
fa.diagram(fit.pc)

fa.parallel(mva)
vss(mva)
corrm.emp <- cor(mva)
corrm.emp
plot(corrm.emp)
mva_pca <- prcomp(mva, scale=TRUE)
plot(mva_pca)

#PCA Summary
#first of all, our analysis shows that we can get rid of some complexity in the data and that instagram/linkedin are highly similar, we can also see that variability mostly comes from our first 6 principal components meaning some of the apps we use dont hold that much weight in terms of usage

#Cluster Summary
#The point of cluster analysis is to group similar data together, so here most app usage is very similar based on student by student data save for one so that's why we have two distinct group for our clustering analysis, it means that one app is being used far more then the others as well since our visualization places cluster 2 farther away


#EFA Summary
#Here we have clearly separated the data into three distinct components based on our columns showing which apps, it seems like the data fund in our well being matches the same amount of time used for linkedin/Reddit since we used a numerical scale for both, also interesting to see that OTT and Twitter/Reddit are used the same amount

#Analysis Takeaway
#Overall this analysis that we've done shows that for the most part there is very little variance between app usage over the multiple apps and types of apps we use, most of the students also use everything very similarly so theres very little global variance except for one app, which is probably OTT


```
